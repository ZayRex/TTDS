{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTDS Lecture 18: Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructor: Björn Ross 17/11/2021\n",
    "\n",
    "Created by Steve Wilson November 2020, modified by Björn Ross November 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build a text classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# some prereqs:\n",
    "import collections\n",
    "\n",
    "# regular expressions\n",
    "import re\n",
    "\n",
    "# for string.punctuation: list of punctuation characters\n",
    "import string\n",
    "\n",
    "# import this for storing our BOW format\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "# scikit learn. Contains lots of ML models we can use\n",
    "# import the library for support vector machines\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# numpy for more easily storing multidimensional data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "* Any package in the Python standard library (https://docs.python.org/3/library/) can be used in the coursework.\n",
    "* Only use sklearn for the classification models! You are **not** allowed to use the `sklearn.feature_extraction` or `sklearn.preprocessing` components for the coursework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Check the data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the data (use ! for command line operation)\n",
    "!cat Tweets.14cat.train | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our data\n",
    "training_data = open('Tweets.14cat.train',encoding=\"latin-1\").read()\n",
    "test_data     = open('Tweets.14cat.test',encoding=\"latin-1\").read()\n",
    "# we will save the testing data for later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of how the tokenization part will work\n",
    "# q: what important features might this remove?\n",
    "invalid_chars = re.compile(f'[{string.punctuation}]')\n",
    "invalid_chars.sub('',\"Hello, World! #Tweets\").lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to list of lists: documents containing tokens\n",
    "# and return the list of categories\n",
    "# also get the vocabulary\n",
    "def preprocess_data(data):\n",
    "    \n",
    "    chars_to_remove = re.compile(f'[{string.punctuation}]')\n",
    "    \n",
    "    documents = []\n",
    "    categories = []\n",
    "    vocab = set([])\n",
    "    \n",
    "    lines = data.split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        # make a dictionary for each document\n",
    "        # word_id -> count (could also be tf-idf score, etc.)\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            # split on tabs, we have 3 columns in this tsv format file\n",
    "            tweet_id, tweet, category = line.split('\\t')\n",
    "\n",
    "            # process the words\n",
    "            words = chars_to_remove.sub('',tweet).lower().split()\n",
    "            for word in words:\n",
    "                vocab.add(word)\n",
    "            # add the list of words to the documents list\n",
    "            documents.append(words)\n",
    "            # add the category to the categories list\n",
    "            categories.append(category)\n",
    "            \n",
    "    return documents, categories, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# ^ see how long this takes\n",
    "# preprocess the data\n",
    "preprocessed_training_data, training_categories, train_vocab = preprocess_data(training_data)\n",
    "preprocessed_test_data, test_categories, test_vocab = preprocess_data(test_data)\n",
    "\n",
    "print(f\"Training Data has {len(preprocessed_training_data)} \" +\n",
    "      f\"documents and vocab size of {len(train_vocab)}\")\n",
    "print(f\"Test Data has {len(preprocessed_test_data)} \" +\n",
    "      f\"documents and vocab size of {len(test_vocab)}\")\n",
    "print(f\"There were {len(set(training_categories))} \" +\n",
    "      f\"categories in the training data and {len(set(test_categories))} in the test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the most common categories in the training data\n",
    "print(collections.Counter(training_categories).most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Set up mappings for word and category IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the vocab to a word id lookup dictionary\n",
    "# anything not in this will be considered \"out of vocabulary\" OOV\n",
    "word2id = {}\n",
    "for word_id,word in enumerate(train_vocab):\n",
    "    word2id[word] = word_id\n",
    "    \n",
    "# and do the same for the categories\n",
    "cat2id = {}\n",
    "for cat_id,cat in enumerate(set(training_categories)):\n",
    "    cat2id[cat] = cat_id\n",
    "    \n",
    "print(\"The word id for dog is\",word2id['dog'])\n",
    "print(\"The category id for Pets & Animals is\",cat2id['Pets & Animals'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Convert data to bag-of-words format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a BOW representation of the files: use the scipy \n",
    "# data is the preprocessed_data\n",
    "# word2id maps words to their ids\n",
    "def convert_to_bow_matrix(preprocessed_data, word2id):\n",
    "    \n",
    "    # matrix size is number of docs x vocab size + 1 (for OOV)\n",
    "    matrix_size = (len(preprocessed_data),len(word2id)+1)\n",
    "    oov_index = len(word2id)\n",
    "    # matrix indexed by [doc_id, token_id]\n",
    "    X = scipy.sparse.dok_matrix(matrix_size)\n",
    "\n",
    "    # iterate through all documents in the dataset\n",
    "    for doc_id,doc in enumerate(preprocessed_data):\n",
    "        for word in doc:\n",
    "            # default is 0, so just add to the count for this word in this doc\n",
    "            # if the word is oov, increment the oov_index\n",
    "            X[doc_id,word2id.get(word,oov_index)] += 1\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "X_train = convert_to_bow_matrix(preprocessed_training_data, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some docs\n",
    "print(\"First 3 documents are:\",X_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the labels to predict\n",
    "y_train = [cat2id[cat] for cat in training_categories]\n",
    "# check the first 3 categories\n",
    "print(y_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train an SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a model: now that the setup is done, it's a piece of cake!\n",
    "%time\n",
    "# instantiate a linear SVM classification model\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
    "# you can set various model hyperparamters here\n",
    "model = sklearn.svm.LinearSVC(C=1000)\n",
    "# then train the model!\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "sample_text = ['retweet','if','you','are','a','cat','person']\n",
    "# create just a single vector as input (as a 1 x V matrix)\n",
    "sample_x_in = scipy.sparse.dok_matrix((1,len(word2id)+1))\n",
    "for word in sample_text:\n",
    "    sample_x_in[0,word2id[word]] += 1\n",
    "\n",
    "# what does the example document look like?\n",
    "print(sample_x_in)\n",
    "prediction = model.predict(sample_x_in)\n",
    "# what category was predicted?\n",
    "print(\"Prediction was:\",prediction[0])\n",
    "# what category was that?\n",
    "print(cat2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on training data: how well did we fit to the data we trained on?\n",
    "y_train_predictions = model.predict(X_train)\n",
    "\n",
    "# now can compute any metrics we care about. Let's quickly do accuracy\n",
    "def compute_accuracy(predictions, true_values):\n",
    "    num_correct = 0\n",
    "    num_total = len(predictions)\n",
    "    for predicted,true in zip(predictions,true_values):\n",
    "        if predicted==true:\n",
    "            num_correct += 1\n",
    "    return num_correct / num_total\n",
    "\n",
    "accuracy = compute_accuracy(y_train_predictions,y_train)\n",
    "print(\"Accuracy:\",accuracy)\n",
    "# how did we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that a good score? The score can be informative, but it isn't hard to do well on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test data in the same was as training data\n",
    "X_test = convert_to_bow_matrix(preprocessed_test_data, word2id)\n",
    "y_test = [cat2id[cat] for cat in test_categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now evaluate on test data: data the model has NOT seen during training time\n",
    "# make sure you do NOT update the model, only get predictions from it\n",
    "y_test_predictions = model.predict(X_test)\n",
    "accuracy = compute_accuracy(y_test_predictions,y_test)\n",
    "print(\"Accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_names = []\n",
    "for cat,cid in sorted(cat2id.items(),key=lambda x:x[1]):\n",
    "    cat_names.append(cat)\n",
    "print(classification_report(y_test, y_test_predictions, target_names=cat_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what would a simple baseline be? How about most common category from before (Gaming)?\n",
    "# we should *definitely* be doing better than this! Otherwise the model is not helping at all\n",
    "baseline_predictions = [cat2id['Gaming']] * len(y_test)\n",
    "baseline_accuracy = compute_accuracy(baseline_predictions,y_train)\n",
    "print(\"Accuracy:\",baseline_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying a different model...\n",
    "# how about a random forest classifier?\n",
    "%time\n",
    "model = sklearn.ensemble.RandomForestClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_train_predictions = model.predict(X_train)\n",
    "print(\"Train accuracy was:\",compute_accuracy(y_train_predictions,y_train))\n",
    "y_test_predictions = model.predict(X_test)\n",
    "print(\"Test accuracy was:\",compute_accuracy(y_test_predictions,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_names = []\n",
    "for cat,cid in sorted(cat2id.items(),key=lambda x:x[1]):\n",
    "    cat_names.append(cat)\n",
    "print(classification_report(y_test, y_test_predictions, target_names=cat_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Other models to try?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check out all of the multiclass ready models! \n",
    "https://scikit-learn.org/stable/modules/multiclass.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
